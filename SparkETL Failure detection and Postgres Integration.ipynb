{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24adff36",
   "metadata": {},
   "source": [
    "# SparkETL-Failure Detection and Postgres Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f711e0e3",
   "metadata": {},
   "source": [
    "by: Judith Okon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2222d33b",
   "metadata": {},
   "source": [
    "## Project Aim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c2e667",
   "metadata": {},
   "source": [
    "The SparkETL-Failure Detection and Postgres Integration project is a data pipeline built using Apache Spark. Its purpose is to collect data from multiple CSV files, combine relevant columns from each dataset into a single dataset, and add a new column called \"failure status.\" This column indicates whether any failures occurred during the data processing stages of the pipeline.\n",
    "\n",
    "By utilizing the distributed processing capabilities of Apache Spark, the project can efficiently handle large volumes of data. It supports various datasets from different sources, ensuring that only the necessary columns are selected and merged together.\n",
    "\n",
    "One significant aspect of this project is the inclusion of the \"pipeline status\" column. This column is dynamically generated based on predefined conditions. It automatically determines whether a failure happened during the execution of the ETL process. Each row in the resulting dataset is assigned a value of 1 or 0 in the \"pipeline status\" column, representing the presence or absence of a pipeline failure, respectively.\n",
    "\n",
    "After successfully integrating the data and adding the \"failure status\" column, the resulting dataset is loaded into a Postgres database. This step guarantees secure storage and facilitates further analysis, reporting, and decision-making based on the processed data.\n",
    "\n",
    "This entire process is known as ETL. (Extract, Transform and Load).\n",
    "\n",
    "The SparkETL-Failure Detection and Postgres Integration project empowers data engineers to establish a dependable and efficient ETL pipeline, facilitating data consolidation and provides insights into potential pipeline failures. By automating the data collection, transformation, and loading processes, organizations can make informed decisions based on the integrated data stored in the Postgres database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f7b910",
   "metadata": {},
   "source": [
    "# Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b6b9ec",
   "metadata": {},
   "source": [
    "This is the first part of the ETL process.\n",
    "\n",
    "We will commence by extracting the required datasets for this project from various CSV files. The datasets include the `Pipeline data table`, `Maintenance data table`, `Sensor data table`, and `Inspection data table`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea239bd",
   "metadata": {},
   "source": [
    "> To accomplish this, we begin by importing the essential tools and libraries, specifically Apache Spark, which provides the necessary framework for our data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36742505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/Users/Judith/spark')\n",
    "\n",
    "# this library helps Python programs to locate the Spark installation and set up the necessary environment variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a76f6a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# this provides a way to interact with Spark and enables the creation and manipulation of Spark DataFrames and Datasets\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "# the types module contains classes that define the schema or structure of the data in Spark.\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "# the functions module provides a collection of built-in functions that can be used for data transformation,\n",
    "# manipulation, and aggregation in Spark DataFrames\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "# the SQLContext class provides a programming interface for working with structured data using Spark SQL. \n",
    "# It allows you to execute SQL queries on Spark DataFrames and provides various methods for data loading, \n",
    "# saving, and manipulation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d13c9353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--driver-class-path /Users/Judith/postgresql-42.6.0.jar pyspark-shell'\n",
    "\n",
    "# this sets a path for the postgres driver. It will be needed in the loading section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b5cee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/21 12:48:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"ETL Data Pipeline\").getOrCreate()\n",
    "\n",
    "# Create a SparkSession: Initialize a SparkSession to work with Spark. \n",
    "# This will serve as the entry point for interacting with Spark\n",
    "# By creating a SparkSession, you gain access to the Spark functionality \n",
    "# and can perform various operations such as reading data, applying transformations, \n",
    "# executing SQL queries, and writing data. It provides a unified interface to interact \n",
    "# with different data sources and execute distributed computations efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9124ccf5",
   "metadata": {},
   "source": [
    "> Read the csv files. We use the sparksession to read the csv files needed for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc800819",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipeline_df = spark.read.csv('pipeline_data_table.csv', header = True, inferSchema = True)\n",
    "maintenance_df = spark.read.csv('maintenance_data_table.csv', header = True, inferSchema = True)\n",
    "sensor_df = spark.read.csv('sensor_data_table.csv', header = True, inferSchema = True)\n",
    "inspection_df = spark.read.csv('inspection_data_table.csv', header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e297fec",
   "metadata": {},
   "source": [
    "We will view the datasets to see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c04c2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----------------+------------------+---------+--------+------------+\n",
      "|_c0|pipeline_id|        lenght_km|     diamteters_mm|age_years|material|    location|\n",
      "+---+-----------+-----------------+------------------+---------+--------+------------+\n",
      "|  0|       PL-1|3.163867170339976|205.96414132846047|       39|   steel|       lagos|\n",
      "|  1|       PL-2|15.12457355971772| 655.0772854593554|       27| plastic|portharcourt|\n",
      "|  2|       PL-3|43.00378167829604|237.04409028519854|       32|   steel|portharcourt|\n",
      "|  3|       PL-4|78.16478032119788| 611.5345293219675|       19|concrete|       lagos|\n",
      "|  4|       PL-5|76.14520496673097| 603.4796154760029|       41|concrete|       lagos|\n",
      "+---+-----------+-----------------+------------------+---------+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/21 12:48:43 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , pipeline_id, lenght_km, diamteters_mm, age_years, material, location\n",
      " Schema: _c0, pipeline_id, lenght_km, diamteters_mm, age_years, material, location\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/Judith/pipeline_data_table.csv\n"
     ]
    }
   ],
   "source": [
    "pipeline_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ba4ec6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----------+---------------------+-----------------+\n",
      "|_c0|pipeline_id|repair_type|repair_duration_hours|  cost_in_dollars|\n",
      "+---+-----------+-----------+---------------------+-----------------+\n",
      "|  0|       PL-1| preventive|                    8|486.9355980554817|\n",
      "|  1|       PL-2|    routine|                    2|            313.0|\n",
      "|  2|       PL-3|    routine|                   11|            252.0|\n",
      "|  3|       PL-4| preventive|                   23|784.6322648503146|\n",
      "|  4|       PL-5| corrective|                    9|11137.27992806314|\n",
      "+---+-----------+-----------+---------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/21 12:48:44 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , pipeline_id, repair_type, repair_duration_hours, cost_in_dollars\n",
      " Schema: _c0, pipeline_id, repair_type, repair_duration_hours, cost_in_dollars\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/Judith/maintenance_data_table.csv\n"
     ]
    }
   ],
   "source": [
    "maintenance_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51357e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----------+----------+------------------+------------------+------------------+\n",
      "|_c0|sensor_id|pipeline_id|      date|          pressure|       temperature|         flow_rate|\n",
      "+---+---------+-----------+----------+------------------+------------------+------------------+\n",
      "|  0| sensor-2|       PL-1|2016-01-23| 89.16509363947836|26.487407250471655| 8.841277524541173|\n",
      "|  1| sensor-4|       PL-2|2021-05-06|51.422621452961415|22.123033999440032|2.2398605213687572|\n",
      "|  2| sensor-1|       PL-3|2014-08-03| 89.93827002882762|24.376792602298494| 8.798215574407859|\n",
      "|  3| sensor-2|       PL-4|2019-03-26|  91.6988260369984| 24.12896689486083|4.4345016457065976|\n",
      "|  4| sensor-3|       PL-5|2015-01-19| 77.33094021729232|23.286447126274545| 4.723534211586986|\n",
      "+---+---------+-----------+----------+------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/21 12:48:45 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , sensor_id, pipeline_id, date, pressure, temperature, flow_rate\n",
      " Schema: _c0, sensor_id, pipeline_id, date, pressure, temperature, flow_rate\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/Judith/sensor_data_table.csv\n"
     ]
    }
   ],
   "source": [
    "sensor_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85e692a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------------+-----------------+--------------+\n",
      "|_c0|pipeline_id|corrosion_level|deformation_level|leak_detection|\n",
      "+---+-----------+---------------+-----------------+--------------+\n",
      "|  0|       PL-1|              4|                2|           Yes|\n",
      "|  1|       PL-2|              3|                3|            No|\n",
      "|  2|       PL-3|              1|                0|            No|\n",
      "|  3|       PL-4|              2|                4|           Yes|\n",
      "|  4|       PL-5|              2|                3|            No|\n",
      "+---+-----------+---------------+-----------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/21 12:48:45 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , pipeline_id, corrosion_level, deformation_level, leak_detection\n",
      " Schema: _c0, pipeline_id, corrosion_level, deformation_level, leak_detection\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/Judith/inspection_data_table.csv\n"
     ]
    }
   ],
   "source": [
    "inspection_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787bfbde",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68396d72",
   "metadata": {},
   "source": [
    "This is the second part of the ETL process.\n",
    "\n",
    "In this section, we will focus on selecting the important columns from each of the datasets and integrating them into a unified dataset.\n",
    "\n",
    "Additionally, we introduce a new column called `pipeline status` that will play a significant role in our analysis. The `pipeline status` column will exhibit binary values: 1 and 0. A value of 1 indicates a pipeline failure, while a value of 0 signifies the absence of any failures. To determine these values, we will employ specific conditions derived from the existing columns, enabling us to accurately identify and track pipeline failures throughout the data processing stages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e39cf30",
   "metadata": {},
   "source": [
    "> ### We will select the important columns from each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "531df784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting important columns from pipeline_df dataset\n",
    "pipeline_columns = ['pipeline_id','lenght_km', 'diamteters_mm', 'age_years', 'material', 'location']\n",
    "pipeline_selected_df = pipeline_df.select(pipeline_columns)\n",
    "\n",
    "# Selecting important columns from maintenance dataset\n",
    "maintenance_columns = ['pipeline_id', 'repair_type', 'repair_duration_hours']\n",
    "maintenance_selected_df = maintenance_df.select(maintenance_columns)\n",
    "\n",
    "# Selecting important columns from sensor dataset\n",
    "sensor_columns = ['pipeline_id', 'pressure', 'temperature', 'flow_rate']\n",
    "sensor_selected_df = sensor_df.select(sensor_columns)\n",
    "\n",
    "# Selecting important columns from inspection dataset\n",
    "inspection_columns = ['pipeline_id','corrosion_level', 'deformation_level', 'leak_detection']\n",
    "inspection_selected_df = inspection_df.select(inspection_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03efc080",
   "metadata": {},
   "source": [
    "> ### We will unify these selected columns into one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3f049ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new dataset that is made up of selected columns from existing datasets, and joining them based on their \n",
    "# primary key ('pipeline_id')\n",
    "joined_df = pipeline_selected_df \\\n",
    "    .join(maintenance_selected_df, \"pipeline_id\", \"left\") \\\n",
    "    .join(sensor_selected_df, \"pipeline_id\", \"left\") \\\n",
    "    .join(inspection_selected_df, \"pipeline_id\", \"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bf55746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+------------------+---------+--------+------------+-----------+---------------------+------------------+------------------+------------------+---------------+-----------------+--------------+\n",
      "|pipeline_id|        lenght_km|     diamteters_mm|age_years|material|    location|repair_type|repair_duration_hours|          pressure|       temperature|         flow_rate|corrosion_level|deformation_level|leak_detection|\n",
      "+-----------+-----------------+------------------+---------+--------+------------+-----------+---------------------+------------------+------------------+------------------+---------------+-----------------+--------------+\n",
      "|       PL-1|3.163867170339976|205.96414132846047|       39|   steel|       lagos| preventive|                    8| 89.16509363947836|26.487407250471655| 8.841277524541173|              4|                2|           Yes|\n",
      "|       PL-2|15.12457355971772| 655.0772854593554|       27| plastic|portharcourt|    routine|                    2|51.422621452961415|22.123033999440032|2.2398605213687572|              3|                3|            No|\n",
      "|       PL-3|43.00378167829604|237.04409028519854|       32|   steel|portharcourt|    routine|                   11| 89.93827002882762|24.376792602298494| 8.798215574407859|              1|                0|            No|\n",
      "|       PL-4|78.16478032119788| 611.5345293219675|       19|concrete|       lagos| preventive|                   23|  91.6988260369984| 24.12896689486083|4.4345016457065976|              2|                4|           Yes|\n",
      "|       PL-5|76.14520496673097| 603.4796154760029|       41|concrete|       lagos| corrective|                    9| 77.33094021729232|23.286447126274545| 4.723534211586986|              2|                3|            No|\n",
      "+-----------+-----------------+------------------+---------+--------+------------+-----------+---------------------+------------------+------------------+------------------+---------------+-----------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the result\n",
    "joined_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2477aa65",
   "metadata": {},
   "source": [
    "> ### We will create a new column called `pipeline_status` that is based on certain conditions derived from some existing columns in the unified joined_df dataset\n",
    "\n",
    "The selected columns used to determine the values for the `pipeline_status` column are `corrosion_level`, `deformation_level`, `leak_detection`, `age_years`, and `repair_type`. These columns were selected becasue they are the most likely to have an effect on the wear and tear of the pipleline.\n",
    "\n",
    "Ranks will be assigned to the values in these selected columns, with the highest possible sum of rank being 20. If the sum of these rank values for a particular row exceeds 10, the `pipeline_status` value for that row will be set to 1, indicating pipeline failure. On the other hand, if the sum of the ranks for each row is less than or equal to 10, the `pipeline_status` value for that row will be set to 0, indicating no pipeline failure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f7eaa0",
   "metadata": {},
   "source": [
    "### 1. Create new columns that ranks each of the selected  columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edf19fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = joined_df.withColumn('corrosion_rank', \n",
    "                                 when(col('corrosion_level') == 0, 0).\n",
    "                                 when(col('corrosion_level') == 1, 1).\n",
    "                                 when(col('corrosion_level') == 2, 2).\n",
    "                                 when(col('corrosion_level') == 3, 3).\n",
    "                                 when(col('corrosion_level') == 4, 4).\n",
    "                                 when(col('corrosion_level') == 5, 5))\n",
    "\n",
    "joined_df = joined_df.withColumn('deformation_rank', \n",
    "                                 when(col('deformation_level') == 0, 0).\n",
    "                                 when(col('deformation_level') == 1, 1).\n",
    "                                 when(col('deformation_level') == 2, 2).\n",
    "                                 when(col('deformation_level') == 3, 3).\n",
    "                                 when(col('deformation_level') == 4, 4).\n",
    "                                 when(col('deformation_level') == 5, 5))\n",
    "\n",
    "joined_df = joined_df.withColumn('leak_detection_rank', \n",
    "                                 when(col('leak_detection') == 'Yes', 2).\n",
    "                                 when(col('leak_detection') == 'No', 1))\n",
    "                                 \n",
    "joined_df = joined_df.withColumn('age_years_rank', \n",
    "                                 when(col('age_years') <= 10, 1).\n",
    "                                 when(col('age_years') <= 20, 2).\n",
    "                                 when(col('age_years') <= 30, 3).\n",
    "                                 when(col('age_years') <= 40, 4).\n",
    "                                 when(col('age_years') <= 50, 5))\n",
    "\n",
    "joined_df = joined_df.withColumn('repair_type_rank', \n",
    "                                 when(col('repair_type') == 'routine', 1).\n",
    "                                 when(col('repair_type') == 'preventive', 2).\n",
    "                                 when(col('repair_type') == 'corrective', 3))\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "224a7e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+\n",
      "|age_years|age_years_rank|\n",
      "+---------+--------------+\n",
      "|       39|             4|\n",
      "|       27|             3|\n",
      "|       32|             4|\n",
      "|       19|             2|\n",
      "|       41|             5|\n",
      "+---------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let us test with the age_range column to see if the mapping worked properly\n",
    "\n",
    "joined_df.select(\"age_years\", \"age_years_rank\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383d4909",
   "metadata": {},
   "source": [
    "### 2. Create a column that results the sum of all the ranked columns  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae3d8b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = joined_df.withColumn('summed_ranks', col('corrosion_rank') + col('deformation_rank') \n",
    "                                 + col('leak_detection_rank') + col('age_years_rank') + col('repair_type_rank'))\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9708e565",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+-------------------+--------------+----------------+------------+\n",
      "|corrosion_rank|deformation_rank|leak_detection_rank|age_years_rank|repair_type_rank|summed_ranks|\n",
      "+--------------+----------------+-------------------+--------------+----------------+------------+\n",
      "|             4|               2|                  2|             4|               2|          14|\n",
      "|             3|               3|                  1|             3|               1|          11|\n",
      "|             1|               0|                  1|             4|               1|           7|\n",
      "|             2|               4|                  2|             2|               2|          12|\n",
      "|             2|               3|                  1|             5|               3|          14|\n",
      "+--------------+----------------+-------------------+--------------+----------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the result\n",
    "joined_df.select('corrosion_rank','deformation_rank','leak_detection_rank',\n",
    "                 'age_years_rank','repair_type_rank','summed_ranks').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea893d5",
   "metadata": {},
   "source": [
    "### 3. Create the `pipeline_status` column that result 1 if the summed_ranks column is greater than 10 and results 0 if the summed_ranks column is less than 10.\n",
    "\n",
    "In the pipeline_status, \n",
    "\n",
    "- 1 indicates pipeline failure\n",
    "- 0 indicates no pipeline failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c63bdaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = joined_df.withColumn('pipeline_status', when(col('summed_ranks')>10, 1).otherwise(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6885fb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|pipeline_status|\n",
      "+---------------+\n",
      "|              1|\n",
      "|              1|\n",
      "|              0|\n",
      "|              1|\n",
      "|              1|\n",
      "+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#show the result\n",
    "joined_df.select('pipeline_status').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8489e8",
   "metadata": {},
   "source": [
    "### 4. Drop irrelevant columns  \n",
    "\n",
    "The main purpose of creating those new columns was to get the pipeline_status. They all played an important role in deriving the pipeline_status column but we do not need them anymore so they will be removed from our datasets.\n",
    "\n",
    "The columns being dropped are: corrosion_rank, deformation_rank, leak_detection_rank, age_years_rank, repair_type_rank, summed_ranks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "622884ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['corrosion_rank', 'deformation_rank', 'leak_detection_rank', 'age_years_rank',\n",
    "                   'repair_type_rank', 'summed_ranks']\n",
    "\n",
    "joined_df = joined_df.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5ef6c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+------------------+---------+--------+------------+-----------+---------------------+------------------+------------------+------------------+---------------+-----------------+--------------+---------------+\n",
      "|pipeline_id|        lenght_km|     diamteters_mm|age_years|material|    location|repair_type|repair_duration_hours|          pressure|       temperature|         flow_rate|corrosion_level|deformation_level|leak_detection|pipeline_status|\n",
      "+-----------+-----------------+------------------+---------+--------+------------+-----------+---------------------+------------------+------------------+------------------+---------------+-----------------+--------------+---------------+\n",
      "|       PL-1|3.163867170339976|205.96414132846047|       39|   steel|       lagos| preventive|                    8| 89.16509363947836|26.487407250471655| 8.841277524541173|              4|                2|           Yes|              1|\n",
      "|       PL-2|15.12457355971772| 655.0772854593554|       27| plastic|portharcourt|    routine|                    2|51.422621452961415|22.123033999440032|2.2398605213687572|              3|                3|            No|              1|\n",
      "|       PL-3|43.00378167829604|237.04409028519854|       32|   steel|portharcourt|    routine|                   11| 89.93827002882762|24.376792602298494| 8.798215574407859|              1|                0|            No|              0|\n",
      "|       PL-4|78.16478032119788| 611.5345293219675|       19|concrete|       lagos| preventive|                   23|  91.6988260369984| 24.12896689486083|4.4345016457065976|              2|                4|           Yes|              1|\n",
      "|       PL-5|76.14520496673097| 603.4796154760029|       41|concrete|       lagos| corrective|                    9| 77.33094021729232|23.286447126274545| 4.723534211586986|              2|                3|            No|              1|\n",
      "+-----------+-----------------+------------------+---------+--------+------------+-----------+---------------------+------------------+------------------+------------------+---------------+-----------------+--------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the result\n",
    "joined_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813d512e",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db19dfa0",
   "metadata": {},
   "source": [
    "This is the last part of the ETL process.\n",
    "\n",
    "We will be loading our finalized dataset to a database, Postgres Database. \n",
    "\n",
    "This step guarantees secure storage and facilitates further analysis, reporting, and decision-making based on the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2afc9ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in postgres connection details\n",
    "postgres_url = \"jdbc:postgresql://localhost:5432/new_database\"\n",
    "properties = {\n",
    "    \"user\": \"myuser\",\n",
    "    \"password\": \"reallyStrongPwd123\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "028fdc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "joined_df.write \\\n",
    "    .jdbc(url=postgres_url, table=\"pipeline_failure\", mode=\"overwrite\", properties=properties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c5753a",
   "metadata": {},
   "source": [
    "Our database has been successfully loaded in postgres, now we can query it and futher analyze it there."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
